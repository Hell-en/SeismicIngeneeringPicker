{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUJo1-JIvy-T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from src import segyrw\n",
        "from src import dsp\n",
        "from src import plot_seismic\n",
        "from src import plot_maps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Reader:\n",
        "\n",
        "    \"\"\"Class for reading input files: .sgy and .txt with FB data\"\"\"\n",
        "\n",
        "    def __init__(self, path, pick):\n",
        "        self.path = path ## to txt\n",
        "        self.pick = pick ## to all segy\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _gen_set(path, df_header_subset, num_file):\n",
        "        x = segyrw.read_sgy_traces(path, df_header_subset['SOU_X'].astype(int).values) ## тут только от 1го sgy файла данные\n",
        "        y = df_header_subset[df_header_subset['SOU_X'] == num_file]['FB'].astype(int).values ## тут надо взять те где совпадает значение с sgy\n",
        "\n",
        "        x = dsp.normalize_traces_by_std(x, 255, axis=1)\n",
        "        x = dsp.normalize_traces(x, scale_type='std')\n",
        "        x = np.expand_dims(x, axis=-1)\n",
        "\n",
        "        heaviside = to_categorical(y+1, num_classes=x.shape[1])\n",
        "        y_map = np.cumsum(heaviside, axis=1)\n",
        "        heaviside = to_categorical(y-1, num_classes=x.shape[1])\n",
        "        y_zeros = np.fliplr(np.cumsum(np.fliplr(heaviside), axis=1))\n",
        "        y_pick = to_categorical(y, num_classes=x.shape[1])\n",
        "\n",
        "        ### mask\n",
        "        y_mask = np.stack((y_zeros, y_pick, y_map), axis=2)\n",
        "\n",
        "        ### det\n",
        "        y_det = to_categorical(y_pick, num_classes=2)\n",
        "\n",
        "        # ### heavi\n",
        "        heaviside = to_categorical(y, num_classes=x.shape[1])\n",
        "        y_map = np.cumsum(heaviside, axis=1)\n",
        "        y_heavi = to_categorical(y_map, num_classes=2)\n",
        "        return x, y_pick, y_det, y_mask, y_heavi\n",
        "\n",
        "\n",
        "    def _create_df(self):\n",
        "        df_header = pd.read_csv(self.path)\n",
        "\n",
        "        n = round(len(df_header.index)*0.2)  ## amount for train 0.2; changeable\n",
        "        df_header_subset = df_header.sample(n, random_state=37)\n",
        "      #  df_header_subset['FB_NTC'] = np.int32(df_header_subset['FB']*500)\n",
        "\n",
        "        all_inds = df_header.index.values\n",
        "        train_inds = df_header_subset.index.values\n",
        "        test_inds = np.setdiff1d(all_inds, train_inds)\n",
        "\n",
        "        df_header_test = df_header.iloc[test_inds]\n",
        "        df_header_test = df_header.iloc[test_inds].sample(random_state=37) # changed. check if ok\n",
        "       # df_header_test['FB_NTC'] = df_header_test['FB']*500  ## instead i need take last column with T of FB\n",
        "\n",
        "        return df_header_subset, df_header_test\n",
        "\n",
        "\n",
        "    def get_file_names(self,):\n",
        "        txtfiles = []\n",
        "        for file in glob.glob(\"*.sgy\"): # отсечь лишнее\n",
        "            txtfiles.append(file)\n",
        "\n",
        "\n",
        "    def get_sou_num(file_name):\n",
        "      string = string[:-4]  # get rid of .sgy\n",
        "      string = file_name\n",
        "      num = 0\n",
        "      string = re.sub(r'^.*?_', '', string) # delete before _\n",
        "      string = string.replace(re.search(r'(?:_)(.*)', string).group(), '') ## delete afetr _\n",
        "\n",
        "      if string[0] == '-':  #starts with -\n",
        "          num =  -int(string[1:].lstrip('0')) ## then\n",
        "      else:\n",
        "          num = int(string.lstrip('0'))\n",
        "      return num\n",
        "\n",
        "\n",
        "    def generate_data(self):\n",
        "        df_header_subset, df_header_test = Reader.create_df()\n",
        "        list_of_files = Reader.get_file_names()\n",
        "\n",
        "        for i in list_of_files:\n",
        "          num_file = Reader.get_sou_num(i)\n",
        "          x, y_pick, y_det, y_mask, y_heavi = Reader._gen_set(i, df_header_subset, num_file)\n",
        "          x_test, y_pick_test, y_det_test, y_mask_test, y_heavi_test = Reader._gen_set(i, df_header_test, num_file)\n",
        "        return x, y_pick, y_mask, x_test"
      ],
      "metadata": {
        "id": "JSOtahcswKRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZVWsry3wKkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Ibhf90WwKnk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}